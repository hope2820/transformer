#!/bin/bash

# =============================================
# Transformerä»é›¶å®ç° - æ”¯æŒIWSLTæ•°æ®é›†
# =============================================

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONHASHSEED=42
export CUDA_LAUNCH_BLOCKING=1
export PYTHONPATH="$PYTHONPATH:$(pwd)"

# é»˜è®¤é…ç½®
SEED=42
CONFIG_FILE="configs/base.yaml"
DATA_DIR="data"
RESULTS_DIR="results"
CHECKPOINTS_DIR="checkpoints"
USE_IWSLT=false
SRC_LANG="en"
TGT_LANG="de"

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    echo "ä½¿ç”¨æ–¹æ³•: $0 [é€‰é¡¹]"
    echo ""
    echo "é€‰é¡¹:"
    echo "  --seed SEED                  è®¾ç½®éšæœºç§å­ (é»˜è®¤: 42)"
    echo "  --config FILE                é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: configs/base.yaml)"
    echo "  --data-dir DIR               æ•°æ®ç›®å½• (é»˜è®¤: data)"
    echo "  --results-dir DIR            ç»“æœç›®å½• (é»˜è®¤: results)"
    echo "  --checkpoints-dir DIR        æ£€æŸ¥ç‚¹ç›®å½• (é»˜è®¤: checkpoints)"
    echo "  --use-iwslt                  ä½¿ç”¨IWSLTæ•°æ®é›†è€Œä¸æ˜¯æœ¬åœ°æ•°æ®"
    echo "  --src-lang LANG              æºè¯­è¨€ä»£ç  (é»˜è®¤: en)"
    echo "  --tgt-lang LANG              ç›®æ ‡è¯­è¨€ä»£ç  (é»˜è®¤: de)"
    echo "  --help                       æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯"
    echo ""
    echo "è¯­è¨€å¯¹ç¤ºä¾‹:"
    echo "  --src-lang en --tgt-lang de  è‹±è¯­åˆ°å¾·è¯­"
    echo "  --src-lang en --tgt-lang fr  è‹±è¯­åˆ°æ³•è¯­"
    echo "  --src-lang de --tgt-lang en  å¾·è¯­åˆ°è‹±è¯­"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 --use-iwslt --src-lang en --tgt-lang de"
    echo "  $0 --config configs/iwslt.yaml"
    echo "  $0 --debug"
}

# è§£æå‘½ä»¤è¡Œå‚æ•°
while [[ $# -gt 0 ]]; do
    case $1 in
        --seed)
            SEED="$2"
            shift 2
            ;;
        --config)
            CONFIG_FILE="$2"
            shift 2
            ;;
        --data-dir)
            DATA_DIR="$2"
            shift 2
            ;;
        --results-dir)
            RESULTS_DIR="$2"
            shift 2
            ;;
        --checkpoints-dir)
            CHECKPOINTS_DIR="$2"
            shift 2
            ;;
        --use-iwslt)
            USE_IWSLT=true
            shift
            ;;
        --src-lang)
            SRC_LANG="$2"
            shift 2
            ;;
        --tgt-lang)
            TGT_LANG="$2"
            shift 2
            ;;
        --help)
            show_help
            exit 0
            ;;
        --debug)
            DEBUG_MODE=true
            shift
            ;;
        *)
            log_error "æœªçŸ¥å‚æ•°: $1"
            show_help
            exit 1
            ;;
    esac
done

log_info "å¼€å§‹è¿è¡ŒTransformerå®éªŒ"
log_info "éšæœºç§å­: $SEED"
log_info "é…ç½®æ–‡ä»¶: $CONFIG_FILE"
log_info "æ•°æ®ç›®å½•: $DATA_DIR"
log_info "ç»“æœç›®å½•: $RESULTS_DIR"
log_info "IWSLTæ¨¡å¼: $USE_IWSLT"
if [ "$USE_IWSLT" = true ]; then
    log_info "ç¿»è¯‘æ–¹å‘: $SRC_LANG -> $TGT_LANG"
fi

# åˆ›å»ºç›®å½•ç»“æ„
create_directories() {
    log_info "åˆ›å»ºç›®å½•ç»“æ„..."

    mkdir -p $DATA_DIR
    mkdir -p $RESULTS_DIR
    mkdir -p $CHECKPOINTS_DIR
    mkdir -p logs
    mkdir -p configs

    log_success "ç›®å½•åˆ›å»ºå®Œæˆ"
}

# æ£€æŸ¥CUDAå¯ç”¨æ€§
check_cuda() {
    if command -v nvidia-smi &> /dev/null; then
        if nvidia-smi | grep -q "NVIDIA-SMI"; then
            log_success "æ£€æµ‹åˆ°NVIDIA GPU"
            CUDA_AVAILABLE=true
        else
            log_warning "æœªæ£€æµ‹åˆ°å¯ç”¨çš„NVIDIA GPUï¼Œå°†ä½¿ç”¨CPU"
            CUDA_AVAILABLE=false
        fi
    else
        log_warning "æœªå®‰è£…nvidia-smiï¼Œå°†ä½¿ç”¨CPU"
        CUDA_AVAILABLE=false
    fi
}

# å®‰è£…ä¾èµ–
install_dependencies() {
    log_info "å®‰è£…Pythonä¾èµ–..."

    # å®‰è£…åŸºç¡€ä¾èµ–
    pip install numpy matplotlib tqdm pyyaml

    # å®‰è£…PyTorchï¼ˆæ ¹æ®CUDAå¯ç”¨æ€§é€‰æ‹©ç‰ˆæœ¬ï¼‰
    if [ "$CUDA_AVAILABLE" = true ]; then
        log_info "å®‰è£…PyTorch (CUDAç‰ˆæœ¬)..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    else
        log_info "å®‰è£…PyTorch (CPUç‰ˆæœ¬)..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
    fi

    # å®‰è£…æ•°æ®é›†ç›¸å…³ä¾èµ–
    log_info "å®‰è£…æ•°æ®é›†å¤„ç†åº“..."
    pip install datasets huggingface-hub sacremoses

    log_success "ä¾èµ–å®‰è£…å®Œæˆ"
}

# å‡†å¤‡IWSLTé…ç½®
setup_iwslt_config() {
    if [ "$USE_IWSLT" = true ]; then
        log_info "è®¾ç½®IWSLTé…ç½®æ–‡ä»¶..."

        # åˆ›å»ºIWSLTä¸“ç”¨é…ç½®
        cat > configs/iwslt.yaml << EOF
# IWSLT 2017æ•°æ®é›†é…ç½®
model:
  d_model: 256
  num_heads: 8
  num_layers: 3
  d_ff: 1024
  dropout: 0.1
  max_seq_length: 128

training:
  batch_size: 32
  num_epochs: 100
  learning_rate: 0.0003
  weight_decay: 0.01
  max_grad_norm: 1.0
  scheduler_step_size: 20
  scheduler_gamma: 0.5

data:
  path: "$DATA_DIR"
  vocab_size: 10000
  use_iwslt: true
  src_lang: "$SRC_LANG"
  tgt_lang: "$TGT_LANG"

experiment:
  seed: $SEED
  data_dir: "$DATA_DIR"
  results_dir: "$RESULTS_DIR"
  checkpoints_dir: "$CHECKPOINTS_DIR"
  log_interval: 100
  save_interval: 10
EOF

        CONFIG_FILE="configs/iwslt.yaml"
        log_success "IWSLTé…ç½®æ–‡ä»¶å·²åˆ›å»º: $CONFIG_FILE"
    fi
}

# å‡†å¤‡æ•°æ®
prepare_data() {
    log_info "å‡†å¤‡æ•°æ®é›†..."

    if [ "$USE_IWSLT" = true ]; then
        log_info "ä½¿ç”¨IWSLT 2017æ•°æ®é›† ($SRC_LANG -> $TGT_LANG)"
        log_warning "æ³¨æ„: IWSLTæ•°æ®é›†å°†ä»Hugging Faceä¸‹è½½ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´"
    else
        # æ£€æŸ¥input.txtæ˜¯å¦å­˜åœ¨
        if [ -f "$DATA_DIR/input.txt" ]; then
            log_info "æ‰¾åˆ° input.txt æ–‡ä»¶"
            # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
            file_size=$(stat -c%s "$DATA_DIR/input.txt" 2>/dev/null || stat -f%z "$DATA_DIR/input.txt")
            line_count=$(wc -l < "$DATA_DIR/input.txt")
            word_count=$(wc -w < "$DATA_DIR/input.txt")
            char_count=$(wc -m < "$DATA_DIR/input.txt")

            echo "æ–‡ä»¶ä¿¡æ¯:"
            echo "  - å¤§å°: $file_size å­—èŠ‚"
            echo "  - è¡Œæ•°: $line_count"
            echo "  - å•è¯æ•°: $word_count"
            echo "  - å­—ç¬¦æ•°: $char_count"
        else
            log_warning "æœªæ‰¾åˆ° input.txt æ–‡ä»¶ï¼Œå°†åˆ›å»ºç¤ºä¾‹æ–‡ä»¶"
        fi
    fi

    # è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬
    python src/data_loader.py

    log_success "æ•°æ®å‡†å¤‡å®Œæˆ"
}

# è®­ç»ƒåŸºç¡€æ¨¡å‹
train_baseline() {
    log_info "å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹..."

    local start_time=$(date +%s)

    local train_cmd="python src/train.py \
        --config $CONFIG_FILE \
        --seed $SEED \
        --data-dir $DATA_DIR \
        --results-dir $RESULTS_DIR \
        --checkpoints-dir $CHECKPOINTS_DIR"

    # æ·»åŠ è°ƒè¯•æ¨¡å¼
    if [ "$DEBUG_MODE" = true ]; then
        train_cmd="$train_cmd --debug"
    fi

    eval $train_cmd 2>&1 | tee logs/training_${SEED}_$(date +%Y%m%d_%H%M%S).log

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))

    log_success "æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: $((duration / 60))åˆ†$((duration % 60))ç§’"
}

# è¿è¡Œæ¶ˆèå®éªŒ
run_ablation_study() {
    log_info "å¼€å§‹æ¶ˆèå®éªŒ..."

    local start_time=$(date +%s)

    python src/ablation_study.py \
        --seed $SEED \
        --data-dir $DATA_DIR \
        --results-dir $RESULTS_DIR \
        2>&1 | tee logs/ablation_${SEED}_$(date +%Y%m%d_%H%M%S).log

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))

    log_success "æ¶ˆèå®éªŒå®Œæˆï¼Œè€—æ—¶: $((duration / 60))åˆ†$((duration % 60))ç§’"
}

# ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹
generate_examples() {
    log_info "ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹..."

    # åˆ›å»ºç‹¬ç«‹çš„Pythonè„šæœ¬æ¥ç”Ÿæˆç¤ºä¾‹
    cat > /tmp/generate_examples.py << 'EOF'
import os
import sys
import torch

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('.')

try:
    from src.model import Transformer
    from src.data_loader import TextDataset
    from src.utils import generate_text

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    checkpoint_path = 'checkpoints/best_model.pth'
    if not os.path.exists(checkpoint_path):
        print('æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆå®Œæˆè®­ç»ƒ')
        exit(0)

    # åŠ è½½æ£€æŸ¥ç‚¹è·å–é…ç½®
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = checkpoint['config']
    vocab = checkpoint['vocab']

    # åˆ›å»ºidx2charæ˜ å°„
    idx2char = {idx: char for char, idx in vocab.items()}

    # åˆ›å»ºæ¨¡å‹
    model = Transformer(
        src_vocab_size=len(vocab),
        tgt_vocab_size=len(vocab),
        d_model=config['model']['d_model'],
        num_heads=config['model']['num_heads'],
        num_layers=config['model']['num_layers'],
        d_ff=config['model']['d_ff'],
        dropout=config['model']['dropout']
    )

    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print('=' * 50)
    print('æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹:')
    print('=' * 50)

    # ä¸åŒçš„èµ·å§‹æ–‡æœ¬
    prompts = [
        'To be or not to',
        'Once upon a time',
        'The king said:',
        'Machine learning',
        'Hello world'
    ]

    for prompt in prompts:
        generated = generate_text(model, prompt, vocab, idx2char, max_length=50)
        print(f'è¾“å…¥: \"{prompt}\"')
        print(f'ç”Ÿæˆ: {generated}')
        print('-' * 40)

except Exception as e:
    print(f'ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {e}')
    import traceback
    traceback.print_exc()
EOF

    python /tmp/generate_examples.py
    rm -f /tmp/generate_examples.py

    log_success "æ–‡æœ¬ç”Ÿæˆå®Œæˆ"
}

# åˆ†æå®éªŒç»“æœ
analyze_results() {
    log_info "åˆ†æå®éªŒç»“æœ..."

    # åˆ›å»ºç‹¬ç«‹çš„Pythonè„šæœ¬æ¥åˆ†æç»“æœ
    cat > /tmp/analyze_results.py << 'EOF'
import json
import os
import glob

def analyze_training_results():
    # åˆ†æè®­ç»ƒç»“æœ
    results_dir = 'results'

    # æ£€æŸ¥è®­ç»ƒæ—¥å¿—
    log_files = glob.glob('logs/training_*.log')
    if log_files:
        latest_log = max(log_files, key=os.path.getctime)
        print(f'åˆ†ææœ€æ–°è®­ç»ƒæ—¥å¿—: {latest_log}')

        with open(latest_log, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # æå–æœ€ç»ˆç»“æœ
        final_lines = [line for line in lines if any(x in line for x in ['éªŒè¯æŸå¤±', 'Epoch', 'æœ€ä½³éªŒè¯æŸå¤±', 'è®­ç»ƒå®Œæˆ'])]
        if final_lines:
            print('è®­ç»ƒç»“æœæ‘˜è¦:')
            for line in final_lines[-10:]:  # æ˜¾ç¤ºæœ€å10è¡Œç›¸å…³ç»“æœ
                print(line.strip())

    # æ£€æŸ¥æ¶ˆèå®éªŒç»“æœ
    ablation_file = os.path.join(results_dir, 'ablation_results.json')
    if os.path.exists(ablation_file):
        with open(ablation_file, 'r', encoding='utf-8') as f:
            ablation_results = json.load(f)

        print('\\næ¶ˆèå®éªŒç»“æœ:')
        print('æ¨¡å‹å˜ä½“                éªŒè¯æŸå¤±    å›°æƒ‘åº¦')
        print('-' * 50)

        labels = {
            'full': 'å®Œæ•´æ¨¡å‹',
            'no_pos_encoding': 'æ— ä½ç½®ç¼–ç ',
            'single_head': 'å•å¤´æ³¨æ„åŠ›',
            'no_residual': 'æ— æ®‹å·®è¿æ¥',
            'no_layernorm': 'æ— LayerNorm'
        }

        for model_type, results in ablation_results.items():
            loss = results['final_val_loss']
            ppl = results['final_perplexity']
            label = labels.get(model_type, model_type)
            print(f'{label:20} {loss:.4f}     {ppl:.2f}')

    # æ£€æŸ¥è®­ç»ƒæ›²çº¿å›¾
    curves_file = os.path.join(results_dir, 'training_curves.png')
    if os.path.exists(curves_file):
        print(f'\\nè®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curves_file}')

    ablation_plot = os.path.join(results_dir, 'ablation_comparison.png')
    if os.path.exists(ablation_plot):
        print(f'æ¶ˆèå®éªŒå¯¹æ¯”å›¾å·²ä¿å­˜: {ablation_plot}')

    # æ£€æŸ¥æ£€æŸ¥ç‚¹
    checkpoint_files = glob.glob('checkpoints/*.pth')
    if checkpoint_files:
        print(f'\\næ‰¾åˆ° {len(checkpoint_files)} ä¸ªæ¨¡å‹æ£€æŸ¥ç‚¹')
        for cf in checkpoint_files[-3:]:  # æ˜¾ç¤ºæœ€è¿‘3ä¸ªæ£€æŸ¥ç‚¹
            print(f'  - {os.path.basename(cf)}')

analyze_training_results()
EOF

    python /tmp/analyze_results.py
    rm -f /tmp/analyze_results.py

    log_success "ç»“æœåˆ†æå®Œæˆ"
}

# ç”Ÿæˆå®éªŒæŠ¥å‘Š
generate_report() {
    log_info "ç”Ÿæˆå®éªŒæŠ¥å‘Š..."

    # åˆ›å»ºç‹¬ç«‹çš„Pythonè„šæœ¬æ¥ç”ŸæˆæŠ¥å‘Š
    cat > /tmp/generate_report.py << 'EOF'
import datetime
import os
import json
import glob

def generate_summary_report():
    # è·å–æ•°æ®æ–‡ä»¶ä¿¡æ¯
    data_info = ""
    input_file = 'data/input.txt'
    if os.path.exists(input_file):
        with open(input_file, 'r', encoding='utf-8') as f:
            text = f.read()
        data_info = f"æ•°æ®é›†ä¿¡æ¯: {len(text)} å­—ç¬¦, {len(text.split())} å•è¯"
    else:
        data_info = "æ•°æ®é›†ä¿¡æ¯: ä½¿ç”¨IWSLTæˆ–å†…ç½®ç¤ºä¾‹æ•°æ®"

    # æ£€æŸ¥é…ç½®
    config_info = "ä½¿ç”¨é»˜è®¤é…ç½®"
    config_files = glob.glob('configs/*.yaml')
    if config_files:
        config_info = f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {', '.join([os.path.basename(f) for f in config_files])}"

    report = f'''Transformerä»é›¶å®ç° - å®éªŒæŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
éšæœºç§å­: 42

{data_info}
{config_info}

ç›®å½•ç»“æ„:
- æ•°æ®ç›®å½•: data
- ç»“æœç›®å½•: results
- æ£€æŸ¥ç‚¹ç›®å½•: checkpoints
- æ—¥å¿—ç›®å½•: logs
- é…ç½®ç›®å½•: configs

å®éªŒæ­¥éª¤:
1. âœ… ç¯å¢ƒé…ç½®å’Œä¾èµ–å®‰è£…
2. âœ… æ•°æ®é›†å‡†å¤‡
3. âœ… Transformeræ¨¡å‹è®­ç»ƒ
4. âœ… æ¶ˆèå®éªŒåˆ†æ
5. âœ… æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
6. âœ… ç»“æœåˆ†æå’Œå¯è§†åŒ–

å…³é”®æ–‡ä»¶:
- è®­ç»ƒé…ç½®: configs/*.yaml
- è®­ç»ƒæ—¥å¿—: logs/training_*.log
- æ¶ˆèå®éªŒæ—¥å¿—: logs/ablation_*.log
- è®­ç»ƒæ›²çº¿: results/training_curves.png
- æ¶ˆèå¯¹æ¯”: results/ablation_comparison.png
- æœ€ä½³æ¨¡å‹: checkpoints/best_model.pth

å¤ç°å‘½ä»¤:
./scripts/run.sh --seed 42 --config configs/base.yaml

æ³¨æ„äº‹é¡¹:
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
- å®Œæ•´å®éªŒè¿è¡Œæ—¶é—´å–å†³äºæ•°æ®é›†å¤§å°å’Œç¡¬ä»¶
- æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦ç»†è®­ç»ƒè¿‡ç¨‹
- IWSLTæ•°æ®é›†éœ€è¦ç½‘ç»œè¿æ¥ä¸‹è½½
'''

    # æ·»åŠ æ¶ˆèå®éªŒç»“æœ
    ablation_file = 'results/ablation_results.json'
    if os.path.exists(ablation_file):
        try:
            with open(ablation_file, 'r', encoding='utf-8') as f:
                ablation_results = json.load(f)

            report += '\\næ¶ˆèå®éªŒç»“æœ:\\n'
            report += 'æ¨¡å‹å˜ä½“                éªŒè¯æŸå¤±    å›°æƒ‘åº¦\\n'
            report += '-' * 50 + '\\n'

            labels = {
                'full': 'å®Œæ•´æ¨¡å‹',
                'no_pos_encoding': 'æ— ä½ç½®ç¼–ç ',
                'single_head': 'å•å¤´æ³¨æ„åŠ›',
                'no_residual': 'æ— æ®‹å·®è¿æ¥',
                'no_layernorm': 'æ— LayerNorm'
            }

            for model_type, results in ablation_results.items():
                loss = results['final_val_loss']
                ppl = results['final_perplexity']
                label = labels.get(model_type, model_type)
                report += f'{label:20} {loss:.4f}     {ppl:.2f}\\n'
        except Exception as e:
            report += f'\\næ¶ˆèå®éªŒç»“æœ: æ— æ³•è¯»å– ({e})\\n'

    report_file = 'results/experiment_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f'å®éªŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}')
    print('\\næŠ¥å‘Šæ‘˜è¦:')
    print(report[:500] + '...' if len(report) > 500 else report)

generate_summary_report()
EOF

    python /tmp/generate_report.py
    rm -f /tmp/generate_report.py

    log_success "å®éªŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    log_info "=== Transformerå®éªŒå¼€å§‹ ==="

    # æ‰§è¡Œå„ä¸ªæ­¥éª¤
    create_directories
    check_cuda
    install_dependencies
    setup_iwslt_config
    prepare_data
    train_baseline

    # åªæœ‰åœ¨éIWSLTæ¨¡å¼ä¸‹æ‰è¿è¡Œæ¶ˆèå®éªŒï¼ˆå› ä¸ºIWSLTè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼‰
    if [ "$USE_IWSLT" != true ]; then
        run_ablation_study
    else
        log_info "è·³è¿‡æ¶ˆèå®éªŒï¼ˆIWSLTæ¨¡å¼ï¼‰"
    fi

    generate_examples
    analyze_results
    generate_report

    log_success "=== æ‰€æœ‰å®éªŒæ­¥éª¤å®Œæˆ ==="
    log_info "å®éªŒç»“æœä¿å­˜åœ¨: $RESULTS_DIR"
    log_info "è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨: logs/"
    log_info "æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: $CHECKPOINTS_DIR"

    # æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
    echo ""
    echo "==========================================="
    echo "å®éªŒå®Œæˆæ€»ç»“:"
    echo "==========================================="
    echo "ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿: $RESULTS_DIR/training_curves.png"
    echo "ğŸ”¬ æŸ¥çœ‹æ¶ˆèå®éªŒ: $RESULTS_DIR/ablation_comparison.png"
    echo "ğŸ“ æŸ¥çœ‹å®éªŒæŠ¥å‘Š: $RESULTS_DIR/experiment_report.txt"
    echo "ğŸ¤– æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ: è¿è¡Œ python src/utils.py"
    if [ "$USE_IWSLT" = true ]; then
        echo "ğŸŒ æ•°æ®é›†: IWSLT 2017 ($SRC_LANG -> $TGT_LANG)"
    else
        echo "ğŸ“ æ•°æ®é›†: æœ¬åœ° input.txt"
    fi
    echo ""
    echo "è¦é‡æ–°è¿è¡Œç‰¹å®šæ­¥éª¤ï¼Œå¯ä»¥å•ç‹¬æ‰§è¡Œç›¸åº”çš„å‡½æ•°"
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"