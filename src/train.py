#!/usr/bin/env python3
"""
Transformeråœ¨IWSLT 2017æ•°æ®é›†ä¸Šçš„è®­ç»ƒ
"""

import os
import time
import math
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import sys
import yaml

sys.path.append('.')

from model import Transformer
from data_loader import IWSLTLocalDataset
from utils import set_seed, count_parameters


def create_masks(src, tgt_input, device):
    """åˆ›å»ºæ³¨æ„åŠ›mask"""
    batch_size, src_len = src.shape
    _, tgt_len = tgt_input.shape

    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)

    tgt_pad_mask = (tgt_input != 0).unsqueeze(1).unsqueeze(2)
    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=device)).bool()
    tgt_sub_mask = tgt_sub_mask.unsqueeze(0).unsqueeze(0)

    tgt_pad_mask_expanded = tgt_pad_mask.expand(-1, -1, tgt_len, -1)
    tgt_mask = tgt_pad_mask_expanded & tgt_sub_mask

    return src_mask, tgt_mask


def generate_translation(model, src_text, vocab, idx2char, max_length=100):
    """ç”Ÿæˆç¿»è¯‘"""
    model.eval()
    device = next(model.parameters()).device

    # ç¼–ç æºæ–‡æœ¬
    src_encoded = [vocab.get(char, vocab['<unk>']) for char in src_text]
    src_tensor = torch.tensor(src_encoded).unsqueeze(0).to(device)

    # èµ·å§‹æ ‡è®°
    start_token = vocab.get('<start>', vocab['<unk>'])
    tgt_encoded = [start_token]
    tgt_tensor = torch.tensor(tgt_encoded).unsqueeze(0).to(device)

    generated = ""

    with torch.no_grad():
        for _ in range(max_length):
            src_mask = (src_tensor != 0).unsqueeze(1).unsqueeze(2)
            tgt_mask = create_masks(tgt_tensor, tgt_tensor, device)[1]

            output = model(src_tensor, tgt_tensor, src_mask, tgt_mask)

            # è·å–æœ€åä¸€ä¸ªé¢„æµ‹
            last_logits = output[0, -1, :]
            next_token_id = torch.argmax(last_logits).item()
            next_char = idx2char.get(next_token_id, '<unk>')

            # å¦‚æœé‡åˆ°ç»“æŸæ ‡è®°æˆ–æœªçŸ¥å­—ç¬¦ï¼Œåœæ­¢ç”Ÿæˆ
            if next_char in ['<end>', '<unk>', '.', '!', '?'] and len(generated) > 10:
                break

            generated += next_char
            tgt_encoded.append(next_token_id)
            tgt_tensor = torch.tensor(tgt_encoded).unsqueeze(0).to(device)

    return generated


class IWSLTTrainer:
    def __init__(self, config):
        self.config = config
        self.device = self._setup_device()

        # ä»åµŒå¥—é…ç½®ä¸­è·å–ç§å­å€¼
        seed = config.get('experiment', {}).get('seed', 42)
        set_seed(seed)

        self._setup_data()
        self._setup_model()
        self._setup_optimizer()

        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.train_losses = []
        self.val_losses = []
        self.perplexities = []

        print(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {count_parameters(self.model):,}")

    def _setup_device(self):
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        else:
            device = torch.device('cpu')
            print("ä½¿ç”¨CPU")
        return device

    def _setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print("åˆå§‹åŒ–IWSLTæ•°æ®åŠ è½½å™¨...")

        # ä»åµŒå¥—é…ç½®ä¸­è·å–å‚æ•°
        data_config = self.config.get('data', {})
        model_config = self.config.get('model', {})
        training_config = self.config.get('training', {})

        data_path = data_config.get('path', 'data')
        max_length = model_config.get('max_seq_length', 128)
        vocab_size = data_config.get('vocab_size', 10000)
        src_lang = data_config.get('src_lang', 'en')
        tgt_lang = data_config.get('tgt_lang', 'de')
        batch_size = training_config.get('batch_size', 32)

        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(data_path, exist_ok=True)

        self.train_dataset = IWSLTLocalDataset(
            data_path=data_path,
            split='train',
            max_length=max_length,
            vocab_size=vocab_size,
            src_lang=src_lang,
            tgt_lang=tgt_lang
        )

        # éªŒè¯é›†ä½¿ç”¨è®­ç»ƒé›†çš„è¯æ±‡è¡¨ï¼Œç¡®ä¿ä¸€è‡´æ€§
        self.val_dataset = IWSLTLocalDataset(
            data_path=data_path,
            split='val',
            max_length=max_length,
            vocab_size=vocab_size,
            src_lang=src_lang,
            tgt_lang=tgt_lang,
            vocab=self.train_dataset.vocab,
            idx2char=self.train_dataset.idx2char
        )

        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=batch_size,
            shuffle=True
        )
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=batch_size
        )

        # ä¿å­˜è¯æ±‡è¡¨ä¿¡æ¯
        self.vocab = self.train_dataset.vocab
        self.idx2char = self.train_dataset.idx2char

    def _setup_model(self):
        print("åˆå§‹åŒ–Transformeræ¨¡å‹...")

        # ä»åµŒå¥—é…ç½®ä¸­è·å–æ¨¡å‹å‚æ•°
        model_config = self.config.get('model', {})
        data_config = self.config.get('data', {})

        src_lang = data_config.get('src_lang', 'en')
        tgt_lang = data_config.get('tgt_lang', 'de')

        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ä½¿ç”¨ç›¸åŒçš„è¯æ±‡è¡¨
        # å¯¹äºçœŸæ­£çš„ç¿»è¯‘ä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦ä¸ºæ¯ç§è¯­è¨€ä½¿ç”¨ä¸åŒçš„è¯æ±‡è¡¨
        vocab_size = len(self.vocab)

        self.model = Transformer(
            src_vocab_size=vocab_size,
            tgt_vocab_size=vocab_size,
            d_model=model_config.get('d_model', 256),
            num_heads=model_config.get('num_heads', 8),
            num_layers=model_config.get('num_layers', 3),
            d_ff=model_config.get('d_ff', 1024),
            max_seq_length=model_config.get('max_seq_length', 128),
            dropout=model_config.get('dropout', 0.1)
        ).to(self.device)

    def _setup_optimizer(self):
        # ä»åµŒå¥—é…ç½®ä¸­è·å–è®­ç»ƒå‚æ•°
        training_config = self.config.get('training', {})

        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=training_config.get('learning_rate', 0.0003),
            weight_decay=training_config.get('weight_decay', 0.01)
        )

        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=training_config.get('scheduler_step_size', 10),
            gamma=training_config.get('scheduler_gamma', 0.5)
        )

        self.criterion = nn.CrossEntropyLoss(ignore_index=0)

    def train_epoch(self):
        self.model.train()
        total_loss = 0

        # ä»åµŒå¥—é…ç½®ä¸­è·å–è®­ç»ƒå‚æ•°
        training_config = self.config.get('training', {})
        max_grad_norm = training_config.get('max_grad_norm', 1.0)

        progress_bar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch + 1}")

        for batch_idx, (src, tgt) in enumerate(progress_bar):
            src, tgt = src.to(self.device), tgt.to(self.device)

            tgt_input = tgt[:, :-1]
            tgt_target = tgt[:, 1:]

            src_mask, tgt_mask = create_masks(src, tgt_input, self.device)

            self.optimizer.zero_grad()
            output = self.model(src, tgt_input, src_mask, tgt_mask)

            loss = self.criterion(
                output.contiguous().view(-1, output.size(-1)),
                tgt_target.contiguous().view(-1)
            )

            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
            self.optimizer.step()

            total_loss += loss.item()

            if batch_idx % 10 == 0:
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{total_loss / (batch_idx + 1):.4f}'
                })

        return total_loss / len(self.train_loader)

    def validate(self):
        self.model.eval()
        total_loss = 0

        with torch.no_grad():
            for src, tgt in tqdm(self.val_loader, desc="éªŒè¯"):
                src, tgt = src.to(self.device), tgt.to(self.device)

                tgt_input = tgt[:, :-1]
                tgt_target = tgt[:, 1:]

                src_mask, tgt_mask = create_masks(src, tgt_input, self.device)
                output = self.model(src, tgt_input, src_mask, tgt_mask)

                loss = self.criterion(
                    output.contiguous().view(-1, output.size(-1)),
                    tgt_target.contiguous().view(-1)
                )

                total_loss += loss.item()

        avg_loss = total_loss / len(self.val_loader)
        perplexity = math.exp(avg_loss)

        return avg_loss, perplexity

    def save_checkpoint(self, filename):
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'perplexities': self.perplexities,
            'best_val_loss': self.best_val_loss,
            'vocab': self.vocab,
            'idx2char': self.idx2char,
            'config': self.config
        }

        # ä»åµŒå¥—é…ç½®ä¸­è·å–æ£€æŸ¥ç‚¹ç›®å½•
        experiment_config = self.config.get('experiment', {})
        checkpoints_dir = experiment_config.get('checkpoints_dir', 'checkpoints_iwslt')

        os.makedirs(checkpoints_dir, exist_ok=True)
        torch.save(checkpoint, f'{checkpoints_dir}/{filename}')
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoints_dir}/{filename}")

    def train(self, num_epochs=None):
        # ä»åµŒå¥—é…ç½®ä¸­è·å–è®­ç»ƒå‚æ•°
        training_config = self.config.get('training', {})
        experiment_config = self.config.get('experiment', {})

        if num_epochs is None:
            num_epochs = training_config.get('num_epochs', 50)

        save_interval = experiment_config.get('save_interval', 10)
        log_interval = experiment_config.get('log_interval', 100)

        print(f"å¼€å§‹è®­ç»ƒ {num_epochs} ä¸ªepoch...")
        start_time = time.time()

        for epoch in range(num_epochs):
            self.current_epoch = epoch

            print(f"\nEpoch {epoch + 1}/{num_epochs}")
            print("-" * 50)

            train_loss = self.train_epoch()
            val_loss, perplexity = self.validate()

            self.scheduler.step()

            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.perplexities.append(perplexity)

            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"å›°æƒ‘åº¦: {perplexity:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint('best_iwslt_model.pth')
                print(f"ğŸ”¥ æ–°çš„æœ€ä½³æ¨¡å‹! éªŒè¯æŸå¤±: {val_loss:.4f}")

            # æŒ‰é…ç½®é—´éš”ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % save_interval == 0:
                self.save_checkpoint(f'checkpoint_epoch_{epoch + 1}.pth')
                self.generate_examples()

        total_time = time.time() - start_time
        print(f"\nè®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time / 60:.2f}åˆ†é’Ÿ")

        self.save_checkpoint('final_iwslt_model.pth')
        self.plot_results()

    def generate_examples(self):
        """ç”Ÿæˆç¿»è¯‘ç¤ºä¾‹"""
        self.model.eval()

        print("\nç¿»è¯‘ç¤ºä¾‹:")
        print("-" * 50)

        # ç¤ºä¾‹æºæ–‡æœ¬
        test_sentences = [
            "Hello, how are you?",
            "What is your name?",
            "The weather is nice today.",
            "I love machine learning.",
            "This is a test sentence."
        ]

        with torch.no_grad():
            for src_text in test_sentences:
                translation = generate_translation(
                    self.model, src_text, self.vocab, self.idx2char
                )
                print(f"æº: {src_text}")
                print(f"è¯‘: {translation}")
                print("-" * 30)

    def plot_results(self):
        """ç»˜åˆ¶è®­ç»ƒç»“æœ"""
        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(self.train_losses, label='è®­ç»ƒæŸå¤±')
        plt.plot(self.val_losses, label='éªŒè¯æŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.title('è®­ç»ƒæ›²çº¿')

        plt.subplot(1, 2, 2)
        plt.plot(self.perplexities)
        plt.xlabel('Epoch')
        plt.ylabel('Perplexity')
        plt.title('éªŒè¯é›†å›°æƒ‘åº¦')

        plt.tight_layout()

        # ä»åµŒå¥—é…ç½®ä¸­è·å–ç»“æœç›®å½•
        experiment_config = self.config.get('experiment', {})
        results_dir = experiment_config.get('results_dir', 'results_iwslt')
        os.makedirs(results_dir, exist_ok=True)

        plt.savefig(f'{results_dir}/iwslt_training_results.png', dpi=300, bbox_inches='tight')
        plt.show()

        print(f"è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {results_dir}/iwslt_training_results.png")


def load_config(config_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main():
    # åŠ è½½YAMLé…ç½®
    config_path = "configs/iwslt.yaml"
    if not os.path.exists(config_path):
        print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = {
            'model': {
                'd_model': 256,
                'num_heads': 8,
                'num_layers': 3,
                'd_ff': 1024,
                'max_seq_length': 128,
                'dropout': 0.1
            },
            'training': {
                'batch_size': 32,
                'num_epochs': 50,
                'learning_rate': 0.0003,
                'weight_decay': 0.01,
                'max_grad_norm': 1.0,
                'scheduler_step_size': 10,
                'scheduler_gamma': 0.5
            },
            'data': {
                'path': 'data',
                'vocab_size': 10000,
                'src_lang': 'en',
                'tgt_lang': 'de'
            },
            'experiment': {
                'seed': 42,
                'data_dir': 'data',
                'results_dir': 'results_iwslt',
                'checkpoints_dir': 'checkpoints_iwslt',
                'log_interval': 100,
                'save_interval': 10
            }
        }
    else:
        config = load_config(config_path)
        print(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    experiment_config = config.get('experiment', {})
    checkpoints_dir = experiment_config.get('checkpoints_dir', 'checkpoints_iwslt')
    results_dir = experiment_config.get('results_dir', 'results_iwslt')

    os.makedirs(checkpoints_dir, exist_ok=True)
    os.makedirs(results_dir, exist_ok=True)

    # è®­ç»ƒæ¨¡å‹
    trainer = IWSLTTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main()